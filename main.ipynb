{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04703337",
   "metadata": {
    "id": "04703337"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DrZubi/MIDS_DATASCI_207/blob/Priya/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77e9e1",
   "metadata": {
    "id": "cb77e9e1"
   },
   "source": [
    "# **Emotions Decoded: The Power of CNNs in Facial Expression Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12427794",
   "metadata": {
    "id": "12427794"
   },
   "source": [
    "#### ``Rational``\n",
    "Facial expression recognition is a critical aspect of human-computer interaction, enabling machines to understand and respond to human emotions.\n",
    "\n",
    "This project has the **objective** to develop a robust and efficient **Convolutional Neural Network (CNN)** model to accurately classify **facial expressions** into seven distinct categories:\n",
    "- Angry\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Sad\n",
    "- Surprise\n",
    "- Neutral\n",
    "\n",
    "By leveraging the power of deep learning, the model will automatically learn and identify complex patterns in facial images, facilitating improved emotional intelligence in various applications such as social robots, virtual assistants, and mental health monitoring systems.\n",
    "\n",
    "#### ``Motivation``\n",
    "The ability to accurately recognize human emotions through facial expressions has vast implications across multiple domains. Enhancing machine understanding of emotions can lead to more intuitive interactions between humans and machines, improve customer service experiences, enhance security systems, and contribute to psychological research. This project seeks to harness the capabilities of CNNs to push the boundaries of emotion recognition technology, ultimately fostering more empathetic and responsive AI systems.\n",
    "\n",
    "#### ``Data``\n",
    "The dataset used in this project comprises 48x48 pixel grayscale images of faces, each depicting one of seven emotions. The images are pre-processed to ensure that the faces are centered and occupy a similar amount of space within each frame. This standardization aids the CNN in learning consistent features across different expressions. The goal is to categorize each face based on the emotion displayed in the facial expression into one of the predefined seven categories.\n",
    "\n",
    "The dataset is sourced from Kaggle, specifically from the “Challenges in Representation Learning: Facial Expression Recognition Challenge” competition. It provides a comprehensive collection of labeled facial images, essential for training and evaluating the performance of the CNN model. The dataset can be accessed on Kaggole [[Source]](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/data).\n",
    "\n",
    "#### ``Authors``\n",
    "- Serah Almeyda\n",
    "- Priya Iragavarapu\n",
    "- Francesca Scipioni\n",
    "- Sowjanya Yaddanapudi\n",
    "- Omar Zu’bi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3a8c1",
   "metadata": {
    "id": "62f3a8c1"
   },
   "source": [
    "---\n",
    "# **Step 1:** Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urhzwoFIW-wJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3754,
     "status": "ok",
     "timestamp": 1722170064115,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "urhzwoFIW-wJ",
    "outputId": "b9183c85-676f-44c6-9398-bb6a141fa4b0"
   },
   "outputs": [],
   "source": [
    "! pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7c1b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3419,
     "status": "ok",
     "timestamp": 1722170067532,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "bbe7c1b8",
    "outputId": "6c574761-87a9-4661-811f-a82819bb7356"
   },
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import import_ipynb\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "# keras_tuner\n",
    "import keras_tuner as kt\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Custom user defined imports\n",
    "#from camera_feed import live_feed #Uncomment if running locally\n",
    "\n",
    "# Inline and set seeds\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5feb3",
   "metadata": {
    "id": "f3c5feb3"
   },
   "source": [
    "---\n",
    "# **Step 2:**  Loading and Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabcff54",
   "metadata": {
    "id": "eabcff54"
   },
   "source": [
    "In this section, we will:  \n",
    "- Load the dataset into a pandas DataFrame.  \n",
    "- Understand the size and structure of the dataset.  \n",
    "- Identify the unique emotion categories present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oCJfck0RelEN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25666,
     "status": "ok",
     "timestamp": 1722170093194,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "oCJfck0RelEN",
    "outputId": "5945247e-8780-4cde-e757-1873d49139aa"
   },
   "outputs": [],
   "source": [
    "# To run on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#current_dir = os.getcwd() #Uncomment if you want to run this locally\n",
    "current_dir = '/content/drive/MyDrive/Colab Notebooks/MIDS_DATASCI_207/'\n",
    "\n",
    "# Import dataset\n",
    "file_path = os.path.join(current_dir, 'Data/icml_face_data.csv')\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4409f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1722170094875,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "7bb4409f",
    "outputId": "5dd37dd2-3c98-4a5c-a4ef-0e14a7c2264f"
   },
   "outputs": [],
   "source": [
    "# Display the shape of the dataset to understand its size (rows, columns)\n",
    "print(f'Dataset size: {df.shape}')\n",
    "\n",
    "# Display the unique emotion categories present in the dataset\n",
    "unique_emotions = np.sort(df['emotion'].unique())\n",
    "print(f'Unique emotions categories: {unique_emotions}')\n",
    "\n",
    "# Display the first five rows of the DataFrame to get an overview of the data\n",
    "print('\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5f06c",
   "metadata": {
    "id": "73c5f06c"
   },
   "source": [
    "**Comment**  \n",
    "The dataset consists of **35,887 images**, each corresponding to one of seven different facial expression categories. The numbers in parentheses represent the facial expressions’ unique labels:\n",
    "\n",
    "**Emotion Categories:**  \n",
    "1.\tAngry (*0*)  \n",
    "2.\tDisgust (*1*)  \n",
    "3.\tFear (*2*)  \n",
    "4.\tHappy (*3*)  \n",
    "5.\tSad (*4*)  \n",
    "6.\tSurprise (*5*)  \n",
    "7.\tNeutral (*6*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43386de",
   "metadata": {
    "executionInfo": {
     "elapsed": 11198,
     "status": "ok",
     "timestamp": 1722170106070,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "c43386de"
   },
   "outputs": [],
   "source": [
    "# Correct column names by converting to lowercase and removing spaces\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '')\n",
    "\n",
    "# Function to convert pixel strings to numpy arrays\n",
    "def string_to_array(string):\n",
    "    \"\"\"\n",
    "    Convert a string of pixel values to a numpy array.\n",
    "\n",
    "    Args:\n",
    "    string (str): A string of pixel values separated by spaces.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 48x48 numpy array of pixel values.\n",
    "    \"\"\"\n",
    "    return np.array(string.split(), dtype='float32').reshape(48, 48)\n",
    "\n",
    "# Apply the function to convert pixel strings to numpy arrays\n",
    "df['image'] = df['pixels'].apply(string_to_array)\n",
    "\n",
    "# Create a mapping of emotion labels to their corresponding emotion names\n",
    "emotion_mapping = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new column with emotion names\n",
    "df['emotion_label'] = df['emotion'].map(emotion_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464fa64",
   "metadata": {
    "id": "c464fa64"
   },
   "source": [
    "---\n",
    "# **Step 3:** Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc04a5c",
   "metadata": {
    "id": "8dc04a5c"
   },
   "source": [
    "To gain a deeper understanding of the dataset and its characteristics, we will create four visualizations:\n",
    "\n",
    "**1. Histogram of Emotion Label Distribution:**  \n",
    "&nbsp; &nbsp; &nbsp; We will create a histogram to show the distribution of the emotion labels. This helps us to check if the classes are balanced or if there is any significant class imbalance. A balanced dataset is crucial for training a robust model.\n",
    "\n",
    "**2. Sample Images per Emotion:**  \n",
    "&nbsp; &nbsp; &nbsp; We will create a grid of sample images for each emotion category. This visual inspection allows us to see the quality and characteristics of the images corresponding to each emotion. It provides an intuitive understanding of what each emotion looks like in the dataset.\n",
    "\n",
    "**3. Image Variability per Emotion:**  \n",
    "&nbsp; &nbsp; &nbsp; We will plot the variability (standard deviation) of pixel values within each emotion category. This plot will give insights into how much variation there is in the images of each emotion. High variability within an emotion category might indicate diverse expressions or inconsistencies in image quality.\n",
    "\n",
    "**4. PCA (Principal Component Analysis):**  \n",
    "&nbsp; &nbsp; &nbsp; We will apply PCA on the pixel data to reduce its dimensionality and visualize the first two principal components. This can help us see if images of different emotions are separable in lower dimensions. PCA is a powerful tool for understanding the intrinsic structure of the data and can highlight any clustering or overlap between different emotion categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0ff22-02d6-4825-be78-b1abac13ddcc",
   "metadata": {
    "id": "04f0ff22-02d6-4825-be78-b1abac13ddcc"
   },
   "source": [
    "**1. Histogram of Emotion Label Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd78b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1722170106071,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "1ebd78b1",
    "outputId": "07f1aaa9-7c37-4b99-f927-9b1aa84875de"
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create histogram\n",
    "counts, bins, patches = plt.hist(df['emotion_label'],\n",
    "                                 bins=np.arange(8) - 0.5,\n",
    "                                 edgecolor='black',\n",
    "                                 align='mid',\n",
    "                                 color='#4BB4DE',\n",
    "                                 rwidth=0.8)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Emotion',\n",
    "           fontsize=14\n",
    "          )\n",
    "plt.ylabel('Frequency',\n",
    "           fontsize=14\n",
    "          )\n",
    "plt.title('Class Balance of Facial Expressions',\n",
    "          fontsize=18,\n",
    "          fontweight='bold',\n",
    "          pad=20\n",
    "         )\n",
    "\n",
    "# Set x-ticks to show emotion labels\n",
    "plt.xticks(range(7))\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels on top of the bars\n",
    "for count, patch in zip(counts, patches):\n",
    "    height = patch.get_height()\n",
    "    plt.text(patch.get_x() + patch.get_width() / 2,\n",
    "             height,\n",
    "             int(count),\n",
    "             ha='center',\n",
    "             va='bottom')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58216e29",
   "metadata": {
    "id": "58216e29"
   },
   "source": [
    "**Comment**  \n",
    "\n",
    "There is a **severe class imbalance** between the different emotion classes in the dataset. The “Happy” class, with 8,989 instances, is significantly overrepresented compared to the other classes. In stark contrast, the “Disgust” class is particularly underrepresented, with only 547 instances. This means that the “Happy” class has approximately 16 times more samples than the “Disgust” class.\n",
    "\n",
    "Implications of Class Imbalance:\n",
    "\n",
    "1. **Model Bias:**  \n",
    "- When training a model on imbalanced data, the model is likely to become biased towards the majority class (“Happy”). This bias occurs because the model encounters the majority class more frequently, leading it to favor predictions for this class.\n",
    "2. **Poor Performance on Minority Classes:**  \n",
    "- The underrepresented classes, such as “Disgust”, may not be learned effectively by the model. As a result, the model may have poor accuracy and higher error rates when predicting these classes, as it has fewer examples to learn from.\n",
    "3. **Misleading Accuracy:**\n",
    "- Overall accuracy can be misleading in the presence of class imbalance. A high accuracy might simply reflect the model’s ability to predict the majority class correctly, while failing to perform well on the minority classes.\n",
    "4. **Impact on Real-world Applications:**\n",
    "- In real-world applications, such as emotion recognition systems, it is crucial to accurately detect all emotion classes, especially those that are less frequent. Misclassifying or failing to detect certain emotions can lead to significant issues in applications like mental health monitoring, human-computer interaction, and customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af4593-1b34-474f-9bac-071d1f0c80ab",
   "metadata": {
    "id": "19af4593-1b34-474f-9bac-071d1f0c80ab"
   },
   "source": [
    "**2. Sample Images per Emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64139654",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1618,
     "status": "ok",
     "timestamp": 1722170107675,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "64139654",
    "outputId": "e80db5b5-64af-4c7e-f5c9-4dbddedc38b6"
   },
   "outputs": [],
   "source": [
    "# Get unique emotion classes\n",
    "emotion_classes = np.sort(df['emotion'].unique())\n",
    "\n",
    "# Define the number of images to display per emotion class\n",
    "num_images_per_class = 5\n",
    "\n",
    "# Create a figure with a grid of subplots\n",
    "fig, axes = plt.subplots(len(emotion_classes),\n",
    "                         num_images_per_class,\n",
    "                         figsize=(6, 12))\n",
    "\n",
    "# Add a main title to the figure\n",
    "fig.suptitle('Sample Images per Emotion Category',\n",
    "             fontsize=18,\n",
    "              fontweight='bold'\n",
    "            )\n",
    "\n",
    "# Specify seed for consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "# Iterate through each emotion class and plot images\n",
    "for i, emotion_class in enumerate(emotion_classes):\n",
    "\n",
    "    # Select 5 random images from the current emotion class\n",
    "    images = df[df['emotion'] == emotion_class].sample(num_images_per_class, random_state=0)['image'].values\n",
    "\n",
    "    # Plot each image in the grid\n",
    "    for j in range(num_images_per_class):\n",
    "        axes[i, j].imshow(images[j], cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "    # Add emotion label in the middle of each row\n",
    "    middle_index = num_images_per_class // 2\n",
    "    axes[i, middle_index].set_title(f'{emotion_mapping[emotion_class]}', fontsize=12, pad=20, fontweight='bold')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689976ae",
   "metadata": {
    "id": "689976ae"
   },
   "source": [
    "**Comment**  \n",
    "The dataset contains 48x48 pixel grayscale images of faces. The faces have been automatically registered to ensure they are centered and occupy roughly the same amount of space in each image. This standardization helps in maintaining consistency across the dataset, which is crucial for training robust machine learning models.\n",
    "\n",
    "However, upon closer inspection, it is evident that some images not only contain the faces but also include the person’s hands. These additional elements, such as hair, ears, and neck, are also captured within the 48x48 pixel frame. The positioning and presence of these features can potentially enhance the perception of the displayed emotion. For instance, a tilted head or visible neck tension might amplify the expression of emotions like sadness or anger.\n",
    "\n",
    "Implications for Classifier Performance:\n",
    "\n",
    "1. **Influence of Additional Features:**\n",
    "- The inclusion of head elements beyond the facial region might introduce additional features that the classifier can learn from. These features could either aid or hinder the classifier’s ability to accurately detect emotions, depending on their relevance and consistency.\n",
    "2. **Enhanced Emotion Cues:**\n",
    "- Head positions and associated features can provide supplementary cues for emotion detection. For example, a bowed head might emphasize sadness, while an upright and tense posture might underscore anger. The classifier might learn to associate these cues with specific emotions, potentially improving its accuracy.\n",
    "3. **Inconsistent Features:**\n",
    "- On the flip side, the variability in head positions and additional features might introduce noise and inconsistencies, making it harder for the classifier to focus solely on the facial expressions. This variability could potentially degrade the model’s performance if it learns to rely on less consistent features.\n",
    "4. **Impact on Generalization:**\n",
    "- The classifier’s reliance on head positions and additional features might impact its generalization to new data. If new images differ in how much of the head is included, the classifier’s performance could vary. Ensuring the model learns to prioritize facial expressions over less reliable head features is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f942251-5785-44f8-9dd4-3f67aba5eb8d",
   "metadata": {
    "id": "9f942251-5785-44f8-9dd4-3f67aba5eb8d"
   },
   "source": [
    "**3. Image Variability per Emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db4b4f-f0d0-4f87-abf9-82b2bddd084a",
   "metadata": {
    "executionInfo": {
     "elapsed": 2833,
     "status": "ok",
     "timestamp": 1722170118776,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "d8db4b4f-f0d0-4f87-abf9-82b2bddd084a"
   },
   "outputs": [],
   "source": [
    "def prepare_image(df):\n",
    "    # Convert pixels string to numpy array and normalize pixel values\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['pixels'] = df['pixels'].apply(lambda x: np.array(x.split(), dtype='float32') / 255.0)\n",
    "\n",
    "    # Reshape data into the required format\n",
    "    image = np.array(df1['pixels'].tolist()).reshape(-1, 48, 48, 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "image = prepare_image(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0a930-0088-4f7e-93c9-8db9a59dc01d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1722170119335,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "24b0a930-0088-4f7e-93c9-8db9a59dc01d",
    "outputId": "a269dc59-9121-4a58-fe7e-e79f62b0cb73"
   },
   "outputs": [],
   "source": [
    "def plot_image_variability(df, image):\n",
    "    # Calculate the standard deviation of each image\n",
    "    pixel_std = np.std(image, axis=(1, 2, 3))\n",
    "\n",
    "    # Create a DataFrame for standard deviations\n",
    "    df_std = pd.DataFrame({'emotion': df['emotion'], 'pixel_std': pixel_std})\n",
    "\n",
    "    # Calculate the average standard deviation per emotion\n",
    "    std_per_emotion = df_std.groupby('emotion')['pixel_std'].mean()\n",
    "\n",
    "    # Plot the average standard deviation per emotion\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create bar plot\n",
    "    counts = std_per_emotion.values\n",
    "    bins = np.arange(len(std_per_emotion))\n",
    "    patches = plt.bar(bins, counts, edgecolor='#787785', color='#4BB4DE', width=0.9)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Emotion', fontsize=14)\n",
    "    plt.ylabel('Average Standard Deviation', fontsize=14)\n",
    "    plt.title('Average Image Variability (Standard Deviation) per Emotion', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "    # Set x-ticks to show emotion labels\n",
    "    plt.xticks(bins, [emotion_mapping[e] for e in std_per_emotion.index])\n",
    "\n",
    "    # Add gridlines for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add data labels on top of the bars\n",
    "    for count, patch in zip(counts, patches):\n",
    "        height = patch.get_height()\n",
    "        plt.text(patch.get_x() + patch.get_width() / 2,\n",
    "                 height,\n",
    "                 f'{count:.2f}',\n",
    "                 ha='center',\n",
    "                 va='bottom')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_image_variability(df, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15ec0f-4782-436a-8315-8b6a803c89b4",
   "metadata": {
    "id": "6b15ec0f-4782-436a-8315-8b6a803c89b4"
   },
   "source": [
    "**Comment**  \n",
    "The seven emotions (classes) exhibit a uniform average standard deviation in pixel values. This indicates that the variability within each emotion category is consistent across the different emotions. In other words, the dispersion of pixel intensities within each emotion class is similar, suggesting that the images for each emotion have comparable levels of detail and variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f0401-d1f8-47e2-abcc-d0f22dc6858f",
   "metadata": {
    "id": "fe9f0401-d1f8-47e2-abcc-d0f22dc6858f"
   },
   "source": [
    "**4. PCA (Principal Component Analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54914a-28ab-4888-a0e2-d2edd995c614",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 2166,
     "status": "ok",
     "timestamp": 1722170121497,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "3c54914a-28ab-4888-a0e2-d2edd995c614",
    "outputId": "f532645b-bd46-494d-9bd7-b2486967d043"
   },
   "outputs": [],
   "source": [
    "def plot_pca(df, image):\n",
    "    # Flatten the arrays for PCA\n",
    "    flat_pixels = image.reshape(image.shape[0], -1)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(flat_pixels)\n",
    "\n",
    "    # Create a DataFrame for PCA results\n",
    "    df_pca = pd.DataFrame({\n",
    "        'emotion': df['emotion'],\n",
    "        'pca_one': pca_result[:, 0],\n",
    "        'pca_two': pca_result[:, 1]\n",
    "    })\n",
    "\n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(df_pca['pca_one'], df_pca['pca_two'], c=df_pca['emotion'], cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Emotion')\n",
    "    plt.title('PCA of Pixel Data', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('PCA Component 1', fontsize=14)\n",
    "    plt.ylabel('PCA Component 2', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_pca(df, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d3132-fb86-4aff-b580-5a96988cb457",
   "metadata": {
    "id": "198d3132-fb86-4aff-b580-5a96988cb457"
   },
   "source": [
    "**Comment**  \n",
    "\n",
    "The PCA plot demonstrates that the pixel data for different facial expressions exhibit considerable overlap in the first two principal components. This indicates that further analysis, possibly using additional features or more sophisticated models, is required to effectively separate and classify the different emotions.\n",
    "\n",
    "In detail:\n",
    "\n",
    "1. **Dense Cluster:**\n",
    "- The plot shows a dense central cluster with data points spread outwards, indicating that the majority of the variance is captured within this central region.\n",
    "- This clustering suggests that the pixel data for different emotions share significant similarities, making it challenging to separate them distinctly based on the first two principal components.\n",
    "2. **Overlap of Emotions:**\n",
    "- The color bar represents different emotions, with each color corresponding to a unique emotion label.\n",
    "- The colors are intermixed throughout the plot, indicating that the emotions are not easily separable in this reduced two-dimensional space.\n",
    "- This overlap suggests that the pixel intensity patterns for different emotions are not distinct enough to be clearly separated by PCA alone.\n",
    "3. **Variation Spread:**\n",
    "- The spread of points along both PCA components shows the extent of variation in the data.\n",
    "- Although there is some spread, the overall circular shape implies that the principal components do not capture clear linear separations between the emotion classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb106b73",
   "metadata": {
    "id": "cb106b73"
   },
   "source": [
    "---\n",
    "# **Step 4:** Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73c842",
   "metadata": {
    "id": "7f73c842"
   },
   "source": [
    "#### **Class imbalance**\n",
    "\n",
    "We will address the issue of class imbalance. The presence of imbalanced data can significantly hinder the detection of rare classes (emotions) because most classification methods implicitly assume that all classes occur with similar frequency. These methods are typically designed to maximize overall classification accuracy, which can lead to poor performance on underrepresented classes.\n",
    "\n",
    "To correct for class imbalance, we will use oversampling techniques to increase the number of instances in the smallest class. This process involves duplicating samples from the minority class until all classes have a similar number of instances. By doing so, we aim to create a more balanced dataset, which should improve the model’s ability to accurately detect and classify the less frequent emotions.\n",
    "\n",
    "By following these steps, we can mitigate the adverse effects of class imbalance and enhance the model’s overall performance, particularly in detecting and classifying rare emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1722170121497,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "6848f546",
    "outputId": "6e864b0c-5430-4812-98c1-7bffb10704a0"
   },
   "outputs": [],
   "source": [
    "# Identify the size of the smallest class\n",
    "max_size = df['emotion'].value_counts().max()\n",
    "\n",
    "# oversample each class to the size of the smallest class\n",
    "df_balanced = df.groupby('emotion').apply(lambda x: resample(x, replace=True, n_samples=max_size, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(df_balanced['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df81dda",
   "metadata": {
    "id": "6df81dda"
   },
   "source": [
    "#### Test Train Split\n",
    "\n",
    "We will follow the steps below to split the data:\n",
    "\n",
    "1. Normalize the Values in the Image Column:\n",
    "    - Rescale images by dividing each pixel value by 255.0.\n",
    "2. Shuffle Images Before Splitting the Data:\n",
    "    - Ensure that the images are randomly shuffled to avoid any ordering bias during the split.\n",
    "3. Use a 60/20/20 Train/Validation/Test Set Split:\n",
    "    - Split the dataset into training, validation, and test sets in the ratio of 60%, 20%, and 20%, respectively.\n",
    "\n",
    "By following these steps, we ensure that the data is properly normalized, shuffled, and split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21021ed3",
   "metadata": {
    "id": "21021ed3"
   },
   "source": [
    "Reason for doing image augmentation: The quantity and diversity of data gathered significantly impact the results of a CNN model. One can use augmentations to artificially inflate the training dataset by warping the original data so that their label does not change. These augmentations can significantly improve learning results without collecting new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a5867",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1722170121497,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "4f1a5867"
   },
   "outputs": [],
   "source": [
    "def transform_split(images, labels, splits):\n",
    "    \"\"\"Split data into train, validation, and test sets; apply transformations and augmentations\n",
    "\n",
    "    Args:\n",
    "    images (np.ndarray): Images of shape (N, 48, 48, 1)\n",
    "    labels (np.ndarray): Labels of shape (N,)\n",
    "    splits (tuple): 3 values summing to 1 defining split of train, validation, and test sets\n",
    "\n",
    "    Returns:\n",
    "    X_train (np.ndarray): Train images of shape (N_train, 48, 48, 1)\n",
    "    y_train (np.ndarray): Train labels of shape (N_train,)\n",
    "    X_val (np.ndarray): Val images of shape (N_val, 48, 48, 1)\n",
    "    y_val (np.ndarray): Val labels of shape (N_val,)\n",
    "    X_test (np.ndarray): Test images of shape (N_test, 48, 48, 1)\n",
    "    y_test (np.ndarray): Test labels of shape (N_test,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize images to [0, 1] range and ensure numpy array type\n",
    "    images = (images / 255.0).to_numpy()\n",
    "    labels = labels.to_numpy()\n",
    "\n",
    "    # Shuffle data\n",
    "    indices = np.arange(images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Create data splits (training, val, and test sets)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(images,\n",
    "                                                        labels,\n",
    "                                                        test_size = 1 - splits[0],\n",
    "                                                        random_state=1234,\n",
    "                                                        stratify=labels)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp,\n",
    "                                                    test_size=splits[2] / (splits[1] + splits[2]),\n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y_temp)\n",
    "\n",
    "    # Reshape the X_train and X_test data to (48, 48, 1)\n",
    "    X_train = [i.reshape(48, 48, 1) for i in X_train]\n",
    "    X_test = [i.reshape(48, 48, 1) for i in X_test]\n",
    "    X_val = [i.reshape(48, 48, 1) for i in X_val]\n",
    "\n",
    "    # Ensure numpy array\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    X_val = np.array(X_val)\n",
    "\n",
    "    # Print shapes\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('X_val shape:', X_val.shape)\n",
    "    print('y_val shape:', y_val.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf705e70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1722170121941,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "cf705e70",
    "outputId": "20886af8-4cbe-404e-a810-28946b8980a2"
   },
   "outputs": [],
   "source": [
    "# Define train, val, test splits\n",
    "split = (0.6, 0.2, 0.2)\n",
    "\n",
    "# Create train test split with augmented data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = transform_split(\n",
    "    df_balanced['image'],\n",
    "    df_balanced['emotion'],\n",
    "    split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f30fd",
   "metadata": {
    "id": "c31f30fd"
   },
   "source": [
    "---\n",
    "# **Step 5:** Modeling (Without Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c27f9",
   "metadata": {
    "id": "8a4c27f9"
   },
   "source": [
    "### **Step 5A:** Baseline model (Without Data Augmentation)\n",
    "\n",
    "**Objective:** To build and train a CNN model to identify facial expressions based on emotions.  \n",
    "\n",
    "We will implement a CNN classifier using the create_CNN function according to the following guidelines. This model, referred to as “baseline_model,” will serve as our baseline classifier:\n",
    "\n",
    "1. Implement the model using the TensorFlow Keras API and the create_CNN function.\n",
    "2. The model receives input images of size 48 x 48 x 1 (i.e., single grayscale images).\n",
    "3. The input data goes through a specified number of convolutional layers with the following specifications:\n",
    "    - Number of filters: 12 (for each convolutional layer)\n",
    "    - Kernel size: 3 x 3\n",
    "    - Strides: 1, 1\n",
    "    - Padding: ‘same’\n",
    "    - Data format: ‘channels_last’\n",
    "    - Activation function: ‘relu’\n",
    "4. Each convolutional layer is followed by a max-pooling layer with a pool size of 2 x 2. Note: This will reduce the size of the feature maps.\n",
    "5. After the convolutional and max-pooling layers, the model includes a dropout layer with a dropout rate of 0.3.\n",
    "6. The dropout layer is followed by a flattening layer.\n",
    "7. The final layer of the model is the classification head, which includes:\n",
    "    - A dense layer with 128 neurons and ‘relu’ activation\n",
    "    - A dropout layer with a dropout rate of 0.3\n",
    "    - An output dense layer with 10 neurons (corresponding to the number of classes) and ‘softmax’ activation\n",
    "8. Build and compile the model using the Adam optimizer with a learning rate of 0.001. Print a summary of the model.\n",
    "9. Train the model on the (X_train, y_train) data for 20 epochs. Implement early stopping by passing the early_stopping callback to the fit() method as callbacks=[early_stopping].\n",
    "\n",
    "These steps outline the structure and training process of \"baseline_model\" , setting a foundation for further model improvements and comparisons.\n",
    "\n",
    "**Option 1: Baseline with no data augmentation**\n",
    "\n",
    "Pros: high accuracies on both training an validation sets.  \n",
    "Cons: The model is likely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83c100",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1722170121942,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "ed83c100"
   },
   "outputs": [],
   "source": [
    "def create_CNN(n_classes=10,\n",
    "               neurons=128,\n",
    "               dropout_rate=0.3,\n",
    "               num_filters=[12],\n",
    "               filter_sizes=[(3, 3)],\n",
    "               maxpools=[(2, 2)],\n",
    "               activation='relu',\n",
    "               optimizer='adam',\n",
    "               learning_rate=0.001,\n",
    "               metrics=['accuracy'],\n",
    "               conv_layers=1):\n",
    "    \"\"\"\n",
    "    Create a Convolutional Neural Network (CNN) model for image classification (48x48x1).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_classes : int, optional, default=10\n",
    "        Number of classes in the classification task.\n",
    "\n",
    "    neurons : int, optional, default=128\n",
    "        Number of neurons in the fully connected dense layer.\n",
    "\n",
    "    dropout_rate : float, optional, default=0.3\n",
    "        Dropout rate to prevent overfitting.\n",
    "\n",
    "    num_filters : list of int, optional, default=[12]\n",
    "        List of integers specifying the number of filters for each Conv2D layer.\n",
    "\n",
    "    filter_sizes : list of tuples, optional, default=[(3, 3)]\n",
    "        List of tuples specifying the size of filters (height, width) for each Conv2D layer.\n",
    "\n",
    "    maxpools : list of tuples, optional, default=[(2, 2)]\n",
    "        List of tuples specifying the size of max pooling windows (height, width) for each MaxPooling2D layer.\n",
    "\n",
    "    activation : str, optional, default='relu'\n",
    "        Activation function to use in Conv2D and Dense layers.\n",
    "\n",
    "    optimizer : str or tf.keras.optimizers.Optimizer, optional, default='adam'\n",
    "        Optimizer to use for training the model.\n",
    "\n",
    "    learning_rate : float, optional, default=0.001\n",
    "        Learning rate for the optimizer.\n",
    "\n",
    "    metrics : list of str, optional, default=['accuracy']\n",
    "        List of metrics to evaluate during training and testing.\n",
    "\n",
    "    conv_layers : int, optional, default=1\n",
    "        Number of convolutional layers to include in the CNN architecture.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : tf.keras.models.Sequential\n",
    "        Compiled CNN model ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear any existing model in the session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Validate input parameters\n",
    "    if not isinstance(num_filters, list) or not isinstance(filter_sizes, list) or not isinstance(maxpools, list):\n",
    "        raise ValueError(\"num_filters, filter_sizes, and maxpools must be lists.\")\n",
    "\n",
    "    if len(num_filters) != conv_layers or len(filter_sizes) != conv_layers or len(maxpools) != conv_layers:\n",
    "        raise ValueError(\"Lengths of num_filters, filter_sizes, and maxpools must match conv_layers.\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(conv_layers):\n",
    "        model.add(tf.keras.layers.Conv2D(\n",
    "            filters=num_filters[i],\n",
    "            kernel_size=filter_sizes[i],\n",
    "            strides=(1, 1),\n",
    "            padding='same',\n",
    "            data_format='channels_last',\n",
    "            activation=activation,\n",
    "            input_shape=(48, 48, 1) if i == 0 else None  # Input shape specified only for the first layer\n",
    "        ))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=maxpools[i]))\n",
    "\n",
    "    # Add the fully connected layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=activation))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer_instance = tf.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "    model.compile(optimizer=optimizer_instance,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=metrics)\n",
    "\n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30529758",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160484,
     "status": "ok",
     "timestamp": 1722170282424,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "30529758",
    "outputId": "543eb10d-4646-4080-bd13-734ce3b69295"
   },
   "outputs": [],
   "source": [
    "# Define Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy for early stopping\n",
    "    patience=4,  # Stop if no improvement after 4 epochs\n",
    "    mode='max',\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Create the baseline model using default settings\n",
    "baseline_model = create_CNN()\n",
    "\n",
    "# Print the model summary to understand the architecture\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train the model with the training data and validate using the validation data\n",
    "history = baseline_model.fit(X_train, y_train,\n",
    "                             epochs=20,\n",
    "                             batch_size=32,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks = [early_stopping],  # Early stopping callback\n",
    "                             verbose=0)  # Set verbose=1 for detailed logging during training\n",
    "\n",
    "# Evaluate the model performance on the training data\n",
    "train_loss, train_accuracy = baseline_model.evaluate(X_train, y_train, verbose=0)\n",
    "\n",
    "# Evaluate the model performance on the validation data\n",
    "val_loss, val_accuracy = baseline_model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# Print the training and validation accuracy\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Comment on the model's performance based on accuracy\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"The model performs better on the training data than on the validation data, which could indicate overfitting.\")\n",
    "else:\n",
    "    print(\"The model performs better or equally well on the validation data compared to the training data, which is a good sign.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479fc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1296,
     "status": "ok",
     "timestamp": 1722170283717,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "0479fc1c",
    "outputId": "531d1e42-2374-43a5-fbce-8446430f1d49"
   },
   "outputs": [],
   "source": [
    "# Define model_path with a valid file extension (.keras or .h5)\n",
    "model_path = os.path.join(current_dir, 'models', 'baseline_model_no_aug.h5')\n",
    "\n",
    "# Save the entire model to the specified path\n",
    "baseline_model.save(model_path)\n",
    "\n",
    "# Load the saved model and verify it's correctly saved\n",
    "try:\n",
    "    loaded_model = load_model(model_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found. Make sure the model is saved correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413dcbb",
   "metadata": {
    "id": "f413dcbb"
   },
   "source": [
    "### **Step 5B:** Hyperparameter Tuning (Without Data Augmentation)\n",
    "\n",
    "**Objective:** To build and train a CNN model to identify facial expressions based on emotions, defining the model by tuning hyperparameters to optimize its performance.\n",
    "\n",
    "**Model Description:** The model is a Convolutional Neural Network (CNN) designed for image classification tasks. It includes multiple convolutional layers, each followed by batch normalization, max pooling, and dropout layers. The model is finalized with a fully connected dense layer and an output layer with a softmax activation function. The architecture is flexible, allowing for hyperparameters such as the number of filters, kernel sizes, dropout rates, and learning rates to be tuned for optimal performance.\n",
    "\n",
    "**Approach:** We will run the hyperparameter tuning twice: once with data augmentation and once without data augmentation. This approach will allow us to compare the results and determine the impact of data augmentation on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b9b47",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1722170283717,
     "user": {
      "displayName": "Omar Zu'bi",
      "userId": "03617309859459027497"
     },
     "user_tz": -180
    },
    "id": "776b9b47"
   },
   "outputs": [],
   "source": [
    "class CNNHyperModel(HyperModel):\n",
    "    \"\"\"\n",
    "    A hypermodel class for creating a Convolutional Neural Network (CNN) with Keras Tuner.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of the input images (height, width, channels).\n",
    "    n_classes : int\n",
    "        Number of classes for classification.\n",
    "    augmentation : bool\n",
    "        Flag to indicate whether to include data augmentation layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_classes, augmentation=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def build(self, hp):\n",
    "        \"\"\"\n",
    "        Build a CNN model with hyperparameter tuning.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        hp : kerastuner.HyperParameters\n",
    "            Hyperparameters to tune.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        model : tf.keras.models.Sequential\n",
    "            Compiled CNN model with tuned hyperparameters.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # Data augmentation layer\n",
    "        if self.augmentation:\n",
    "            data_augmentation = tf.keras.Sequential([\n",
    "                RandomFlip(\"horizontal\", input_shape=self.input_shape),\n",
    "                RandomRotation(0.2),  # Adjusting width and height by +-20 degrees\n",
    "                RandomZoom(0.15)      # Zooming by +-15 degrees\n",
    "            ])\n",
    "            model.add(data_augmentation)\n",
    "\n",
    "        # First Conv2D layer\n",
    "        model.add(Conv2D(\n",
    "            filters=hp.Int('conv_1_filters', min_value=32, max_value=256, step=32),\n",
    "            kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(0.001),\n",
    "            input_shape=self.input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(hp.Float('conv_1_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Second Conv2D layer\n",
    "        model.add(Conv2D(\n",
    "            filters=hp.Int('conv_2_filters', min_value=32, max_value=256, step=32),\n",
    "            kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(0.001)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(hp.Float('conv_2_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Third Conv2D layer\n",
    "        model.add(Conv2D(\n",
    "            filters=hp.Int('conv_3_filters', min_value=32, max_value=256, step=32),\n",
    "            kernel_size=hp.Choice('conv_3_kernel', values=[3, 5]),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(0.001)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(hp.Float('conv_3_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Flatten layer\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # Dense layer\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('dense_units', min_value=128, max_value=512, step=64),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(0.001)))\n",
    "        model.add(Dropout(hp.Float('dense_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(self.n_classes, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429f79e",
   "metadata": {
    "id": "5429f79e"
   },
   "source": [
    "### Hyperparameter Tuning Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc24464",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cc24464",
    "outputId": "ce56a9f0-288a-4c19-d9ee-db340f6eeb0b"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning setup\n",
    "input_shape = (48, 48, 1)\n",
    "n_classes = 10\n",
    "hypermodel = CNNHyperModel(input_shape, n_classes, augmentation=False)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='cnn_tuning')\n",
    "\n",
    "# Print the search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Perform Tuner Search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=20,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "\n",
    "# Train the final model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# Evaluate the accuracy of the final model on training, validation, and test datasets\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Comment on model performance\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"The model performs better on the training data than on the validation data, which could indicate overfitting.\")\n",
    "else:\n",
    "    print(\"The model performs better or equally well on the validation data compared to the training data, which is a good sign.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152a4d1",
   "metadata": {
    "id": "a152a4d1"
   },
   "outputs": [],
   "source": [
    "# Define model_path with a valid file extension (.keras or .h5)\n",
    "model_path = os.path.join(current_dir, 'models', 'tuned_model_no_augm.h5')\n",
    "\n",
    "# Save the entire model to the specified path\n",
    "model.save(model_path)\n",
    "\n",
    "# Load the saved model and verify it's correctly saved\n",
    "try:\n",
    "    loaded_model = load_model(model_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found. Make sure the model is saved correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a942582",
   "metadata": {
    "id": "2a942582"
   },
   "source": [
    "### **Step 5C:** Model (Without Data Augmentation) Results Analysis\n",
    "We will now use the test data to evaluate the performance (accuracy) on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c53d2",
   "metadata": {
    "id": "1c2c53d2"
   },
   "outputs": [],
   "source": [
    "# Function to compute training and validation accuracy\n",
    "def compute_accuracy(model, history, X_train, y_train, X_test, y_test, epochs):\n",
    "    '''\n",
    "    Computes the training and validation accuracy after fitting, and evaluates the final testing accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: tf.keras.model\n",
    "        The CNN model instance created and compiled after fitting.\n",
    "\n",
    "    history: obj\n",
    "        The history object returned from model.fit(), containing training metrics.\n",
    "\n",
    "    X_train: numpy.ndarray\n",
    "        Training dataset input.\n",
    "\n",
    "    y_train: numpy.ndarray\n",
    "        Training dataset labels.\n",
    "\n",
    "    X_test: numpy.ndarray\n",
    "        Testing dataset input.\n",
    "\n",
    "    y_test: numpy.ndarray\n",
    "        Testing dataset labels.\n",
    "\n",
    "    epochs: int\n",
    "        Number of epochs the model was trained for.\n",
    "    '''\n",
    "\n",
    "    # Get training and valudation accuracy of digits model\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print last epoch results\n",
    "    print('Training accuracy: %1.4f' % train_accuracy[-1])\n",
    "    print('Validation accuracy: %1.4f' % val_accuracy[-1])\n",
    "\n",
    "    # Evaluate testing results\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('\\nTesting loss: %1.4f' % test_loss)\n",
    "    print('Testing accuracy: %1.4f' % test_acc)\n",
    "\n",
    "# Compute Model Accuracies\n",
    "compute_accuracy(model, history, X_train, y_train , X_test, y_test, 25)\n",
    "\n",
    "# Get testing dataset predictions and labels\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds_labels = np.argmax(test_preds, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "emotions = ['Angry','Disgust','Fear','Happy','Sad','Suprise','Neutral']\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d',\n",
    "            xticklabels=emotions,\n",
    "            yticklabels=emotions)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074ee29",
   "metadata": {
    "id": "a074ee29"
   },
   "source": [
    "---\n",
    "# **Step 6:** Modeling (With Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae678f",
   "metadata": {
    "id": "4aae678f"
   },
   "source": [
    "### Step 6A: Baseline Model (With Data Augmentation)\n",
    "\n",
    "\n",
    "**Option 2:** Baseline with Data Augmentation  \n",
    "Pros: The model is not overfitting.  \n",
    "Cons: accuracies on both training an validation sets are lower than the baseline without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42331f-8709-48b9-833e-4a30b9e4143b",
   "metadata": {
    "id": "df42331f-8709-48b9-833e-4a30b9e4143b"
   },
   "outputs": [],
   "source": [
    "# Define Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy for early stopping\n",
    "    patience=4,  # Stop if no improvement after 4 epochs\n",
    "    mode='max',\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Image augmentation (random flip) on training data\n",
    "def augment_images(images):\n",
    "  aug_images = []\n",
    "  for img in images:\n",
    "    img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "    flipped_img = tf.image.flip_left_right(img_tensor)\n",
    "    aug_images.append(img_tensor)\n",
    "    aug_images.append(flipped_img)\n",
    "  return np.array(aug_images)\n",
    "\n",
    "# image augmentation (random flip) on training data\n",
    "X_train_augm = augment_images(X_train)\n",
    "\n",
    "# concatenate original X_train and augmented X_train_augm data\n",
    "X_train_augm = np.concatenate([X_train, X_train_augm], axis=0)\n",
    "\n",
    "# concatenate y_train (note the label is preserved)\n",
    "y_train_augm = y_train\n",
    "y_train_augm = np.concatenate([y_train, y_train_augm],axis=0)\n",
    "y_train_augm = np.concatenate([y_train, y_train_augm],axis=0)\n",
    "\n",
    "# shuffle X_train and y_train, i.e., shuffle two tensors in the same order\n",
    "shuffle = np.random.permutation(len(X_train_augm))\n",
    "X_train_augm = X_train_augm[shuffle]\n",
    "y_train_augm = y_train_augm[shuffle]\n",
    "\n",
    "# Esnure numpy array\n",
    "X_train_augm = np.array(X_train_augm)\n",
    "\n",
    "# Create the baseline model using default settings\n",
    "baseline_model = create_CNN()\n",
    "\n",
    "# Print the model summary to understand the architecture\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train the model with the training data and validate using the validation data\n",
    "history = baseline_model.fit(X_train_augm, y_train_augm,\n",
    "                             epochs=20,\n",
    "                             batch_size=32,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks=[early_stopping],  # Early stopping callback\n",
    "                             verbose=0)  # Set verbose=1 for detailed logging during training\n",
    "\n",
    "# Evaluate the model performance on the training data\n",
    "train_loss, train_accuracy = baseline_model.evaluate(X_train_augm, y_train_augm, verbose=0)\n",
    "\n",
    "# Evaluate the model performance on the validation data\n",
    "val_loss, val_accuracy = baseline_model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# Print the training and validation accuracy\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Comment on the model's performance based on accuracy\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"The model performs better on the training data than on the validation data, which could indicate overfitting.\")\n",
    "else:\n",
    "    print(\"The model performs better or equally well on the validation data compared to the training data, which is a good sign.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1c4eb",
   "metadata": {
    "id": "b1a1c4eb"
   },
   "outputs": [],
   "source": [
    "# Define model_path with a valid file extension (.keras or .h5)\n",
    "model_path = os.path.join(current_dir, 'models', 'baseline_model_augm.h5')\n",
    "\n",
    "# Save the entire model to the specified path\n",
    "baseline_model.save(model_path)\n",
    "\n",
    "# Load the saved model and verify it's correctly saved\n",
    "try:\n",
    "    loaded_model = load_model(model_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found. Make sure the model is saved correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55faf945",
   "metadata": {
    "id": "55faf945"
   },
   "source": [
    "### Step 6B: Hyperparameter Tuning (With Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f72c4",
   "metadata": {
    "id": "bc7f72c4"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning setup\n",
    "input_shape = (48, 48, 1)\n",
    "n_classes = 10\n",
    "hypermodel = CNNHyperModel(input_shape, n_classes, augmentation=True)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='cnn_tuning')\n",
    "\n",
    "# Perform Tuner Search\n",
    "tuner.search_space_summary()\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=20,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "\n",
    "# Train the final model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# Evaluate the accuracy of the final model on (X_train, y_train), (X_val, y_val), and (X_test, y_test)\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Comment on model performance\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"The model performs better on the training data than on the validation data, which could indicate overfitting.\")\n",
    "else:\n",
    "    print(\"The model performs better or equally well on the validation data compared to the training data, which is a good sign.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04158a21",
   "metadata": {
    "id": "04158a21"
   },
   "outputs": [],
   "source": [
    "# Define model_path with a valid file extension (.keras or .h5)\n",
    "model_path = os.path.join(current_dir, 'models', 'tuned_model_augm.h5')\n",
    "\n",
    "# Save the entire model to the specified path\n",
    "model.save(model_path)\n",
    "\n",
    "# Load the saved model and verify it's correctly saved\n",
    "try:\n",
    "    loaded_model = load_model(model_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found. Make sure the model is saved correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0ALxwp9AuB5",
   "metadata": {
    "id": "s0ALxwp9AuB5"
   },
   "source": [
    "### Step 6c: Model (With Data Augmentation) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zYGjpdpv92UF",
   "metadata": {
    "id": "zYGjpdpv92UF"
   },
   "outputs": [],
   "source": [
    "# Compute Model Accuracies\n",
    "compute_accuracy(model, history, X_train, y_train , X_test, y_test, 25)\n",
    "\n",
    "# Get testing dataset predictions and labels\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds_labels = np.argmax(test_preds, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "emotions = ['Angry','Disgust','Fear','Happy','Sad','Suprise','Neutral']\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d',\n",
    "            xticklabels=emotions,\n",
    "            yticklabels=emotions)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c5018",
   "metadata": {
    "id": "4a7c5018"
   },
   "source": [
    "----\n",
    "# Step 7: Model Live Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d3014",
   "metadata": {
    "id": "fc9d3014"
   },
   "outputs": [],
   "source": [
    "live_feed(model) # Type Q to exit out"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
